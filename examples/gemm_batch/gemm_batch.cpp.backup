#include <iostream>
#include <string>
#include <vector>
#include "tuner_api.h"

#define USE_CUDA 1

#if USE_CUDA == 0
#if defined(_MSC_VER)
    #define KTT_KERNEL_FILE "../examples/gemm_batch/gemm_kernel.cl"
#else
    #define KTT_KERNEL_FILE "../../examples/gemm_batch/gemm_kernel.cl"
#endif
#else
#if defined(_MSC_VER)
    #define KTT_KERNEL_FILE "../examples/gemm_batch/gemm_kernel.cu"
#else
    #define KTT_KERNEL_FILE "../../examples/gemm_batch/gemm_kernel.cu"
#endif
#endif

#define REAL float

class referenceGemm : public ktt::ReferenceClass
{
public:
    referenceGemm(const std::vector<REAL>& srcA, const std::vector<REAL>& srcB, const int a, const int b, const int c, const int n, const ktt::ArgumentId resultArgumentId) :
        srcA(srcA),
	    srcB(srcB),
    	a(a),
    	b(b),
	    c(c),
    	n(n),
        resultArgumentId(resultArgumentId)
    {}

    void computeResult() override {
        res.resize(n*c*b);
        for (int i = 0; i < n; i++) {
            for (int j = 0; j < c; j++)
                for (int k = 0; k < b; k++) {
                    REAL tmp = 0.0f;
                    for (int l = 0; l < a; l++)
                        tmp += srcA[i*a*b + k*a + l] * srcB[i*c*a + l*c + j];
                    res[i*c*b + k*c + j] = tmp;
                }
            if (i == 0) {
                for (int bl = 0; bl < b; bl++){
                    for (int al = 0; al < a; al++)
                        std::cout << srcA[i*a*b + bl*a + al] << " ";
                    std::cout << "\n";
                }
                for (int al = 0; al < a; al++){
                    for (int cl = 0; cl < c; cl++)
                        std::cout << srcB[i*c*b + al*c + cl] << " ";
                    std::cout << "\n";
                }
                for (int bl = 0; bl < b; bl++){
                    for (int cl = 0; cl < c; cl++)
                        std::cout << res[i*c*b + bl*c + cl] << " ";
                    std::cout << "\n";
                }
            }
	    }
    }

    void* getData(const ktt::ArgumentId id) override {
        if (id == resultArgumentId) {
            return (void*)res.data();
        }
        return nullptr;
    }

    size_t getNumberOfElements(const ktt::ArgumentId argumentId) const override {
        return n*c*b;
    }

private:
    std::vector<REAL> res;
    std::vector<REAL> srcA;
    std::vector<REAL> srcB;
    int a, b, c, n;
    ktt::ArgumentId resultArgumentId;
};

class cTunableGemm : public ktt::TuningManipulator {
public:
    cTunableGemm(const int batch, const int a, const int b, const int c)  :
        batch(batch),
        a(a),
        b(b),
        c(c)
    {}
    void launchComputation(const ktt::KernelId kernelId) override {
        ktt::DimensionVector globalSize(1, 1, 1);
        ktt::DimensionVector localSize(1, 1, 1);
        std::vector<ktt::ParameterPair> parameterValues = getCurrentConfiguration();
        int gran = getParameterValue("GRANULARITY", parameterValues);
        if (gran == 1) {
#if USE_CUDA == 0
            globalSize.setSizeX(batch);
#else
            globalSize.setSizeX(batch/getParameterValue("GROUP_SIZE_X", parameterValues));
#endif
            localSize.setSizeX(getParameterValue("GROUP_SIZE_X", parameterValues));
        }
        if (gran == 2) {
            int y = getParameterValue("MGCG_GROUP_SIZE_Y", parameterValues);
#if USE_CUDA == 0
            globalSize.setSizeX(batch*c / y);
            globalSize.setSizeY(y);
#else
            globalSize.setSizeX(batch / y);
#endif
            localSize.setSizeX(c);
            localSize.setSizeY(y);
        }
        if (gran == 3) {
            int y = getParameterValue("MGCG_GROUP_SIZE_Y", parameterValues);
#if USE_CUDA == 0
            globalSize.setSizeX(batch*c);
            globalSize.setSizeY(y);
#else
            globalSize.setSizeX(batch);
#endif
            localSize.setSizeX(c);
            localSize.setSizeY(y);
        }

        runKernel(kernelId, globalSize, localSize);
    }
private:
    int batch, a, b, c;
};

int main(int argc, char** argv)
{
    // Initialize platform and device index
    ktt::PlatformIndex platformIndex = 0;
    ktt::DeviceIndex deviceIndex = 0;
    std::string kernelFile = KTT_KERNEL_FILE;

    if (argc >= 2)
    {
        platformIndex = std::stoul(std::string(argv[1]));
        if (argc >= 3)
        {
            deviceIndex = std::stoul(std::string(argv[2]));
            if (argc >= 4)
            {
                kernelFile = std::string(argv[3]);
            }
        }
    }

    // Declare and initialize data (m, n > 1)
    const int a = 3;
    const int b = 3;
    const int c = 3;
    const int batch = 1024*64;
    std::vector<REAL> srcA(a*b*batch, 0.0f);
    std::vector<REAL> srcB(c*a*batch, 0.0f);
    std::vector<REAL> srcAT(b*a*batch, 0.0f);
    std::vector<REAL> dst(c*b*batch, 0.0f);

    // fill with random data
    for (int i = 0; i < a*b*batch; i++)
    {
        srcA[i] = 10.0f*((REAL)rand()) / ((REAL) RAND_MAX);
    }
    for (int i = 0; i < c*a*batch; i++)
    {
        srcB[i] = 10.0f*((REAL)rand()) / ((REAL) RAND_MAX);
    }
    // fill transposed B
    for (int i = 0; i < batch; i++)
        for (int j = 0; j < a; j++)
            for (int k = 0; k < b; k++)
            {
                srcAT[i*b*a + j*b + k] = srcA[i*a*b + k*a + j];
            }

#if USE_CUDA == 0
    ktt::Tuner tuner(platformIndex, deviceIndex);
#else
    ktt::Tuner tuner(0, deviceIndex, ktt::ComputeAPI::CUDA);
#endif

    // create kernel
    ktt::DimensionVector ndRangeDimensions(batch);
    ktt::DimensionVector workGroupDimensions;
    ktt::KernelId kernelId = tuner.addKernelFromFile(kernelFile, "gemm_batch", ndRangeDimensions, workGroupDimensions);

    // create input/output
    ktt::ArgumentId srcAId = tuner.addArgumentVector(srcA, ktt::ArgumentAccessType::ReadOnly);
    ktt::ArgumentId srcBId = tuner.addArgumentVector(srcB, ktt::ArgumentAccessType::ReadOnly);
    ktt::ArgumentId srcATId = tuner.addArgumentVector(srcAT, ktt::ArgumentAccessType::ReadOnly);
    ktt::ArgumentId dstId = tuner.addArgumentVector(dst, ktt::ArgumentAccessType::WriteOnly);
    ktt::ArgumentId nId = tuner.addArgumentScalar(batch);
    tuner.setKernelArguments(kernelId, std::vector<ktt::ArgumentId>{srcAId, srcBId, srcATId, dstId, nId});

    tuner.addParameter(kernelId, "SIZE_A", {a});
    tuner.addParameter(kernelId, "SIZE_B", {b});
    tuner.addParameter(kernelId, "SIZE_C", {c});
//#if 0
    tuner.addParameter(kernelId, "GRANULARITY", {/*1, 2, */2}); // 1 = fine (matrix per thread), 2 = medium (block of a), 3 = coarse (block of a*b)
    tuner.addParameter(kernelId, "GROUP_SIZE_X", {1, 32, 64, 128, 256, 512});
    tuner.addParameter(kernelId, "MGCG_GROUP_SIZE_X", {1, c});
    tuner.addParameter(kernelId, "MGCG_GROUP_SIZE_Y", {1, 2, 4, 8, 16, 32});
    tuner.addParameter(kernelId, "CACHING_STRATEGY", {0, 1, 2}); /* 0 = implicit caching, 1 = local memory, 2 = private memory */
    tuner.addParameter(kernelId, "TRANSPOSED_A", {0, 1});
//#endif
#if 0
    tuner.addParameter(kernelId, "GRANULARITY", {3}); // 1 = fine (matrix per thread), 2 = medium (block of a), 3 = coarse (block of a*b)
    tuner.addParameter(kernelId, "GROUP_SIZE_X", {1, 32, 64, 128, 256, 512});
    tuner.addParameter(kernelId, "MGCG_GROUP_SIZE_X", {1, c});
    tuner.addParameter(kernelId, "MGCG_GROUP_SIZE_Y", {8});
    tuner.addParameter(kernelId, "CACHING_STRATEGY", {1}); /* 0 = implicit caching, 1 = local memory, 2 = private memory */
    tuner.addParameter(kernelId, "TRANSPOSED_B", {0});
#endif

    auto parallelismConstraint = [](std::vector<size_t> v) {return (v[0] == 1 && v[1] > 1 && v[2] == 1 && v[3] == 1) || (v[0] == 2 && v[1] == 1 && v[2] > 1) || (v[0] == 3 && v[1] == 1 && v[2] > 1);};
    tuner.addConstraint(kernelId, parallelismConstraint, {"GRANULARITY", "GROUP_SIZE_X", "MGCG_GROUP_SIZE_X", "MGCG_GROUP_SIZE_Y"});
    auto tmpConstraint = [](std::vector<size_t> v) {return (v[0] < 3 || v[1] < 3);};
    tuner.addConstraint(kernelId, tmpConstraint, {"GRANULARITY", "CACHING_STRATEGY"});

    tuner.setReferenceClass(kernelId, std::make_unique<referenceGemm>(srcA, srcB, a, b, c, batch, dstId), std::vector<ktt::ArgumentId>{dstId});
    tuner.setValidationMethod(ktt::ValidationMethod::SideBySideComparison, 0.001f);
    tuner.setValidationRange(dstId, c*b*batch);

    tuner.setTuningManipulator(kernelId, std::make_unique<cTunableGemm>(batch, a, b, c));
    
    tuner.tuneKernel(kernelId);
    tuner.printResult(kernelId, std::cout, ktt::PrintFormat::Verbose);
    tuner.printResult(kernelId, "gemm_batch_output.csv", ktt::PrintFormat::CSV);

    return 0;
}
