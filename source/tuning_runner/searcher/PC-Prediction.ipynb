{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Result for  DT  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 98.46632265123763\n",
      "             Mean squared error is         :  115552759365665.08\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Result for  Proposed  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 99.96334103168518\n",
      "             Mean squared error is         :  2762018326539.116\n",
      "=======================================================================\n",
      "Proposed  : For  680-bicg_output  and test size  10  === Maximum score between these models are: 0.9996334103168517\n",
      "Total time that elapsed in this expriment is:  462.177\n",
      "=======================================================================\n",
      "Training Result for  DT  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 98.93384064654752\n",
      "             Mean squared error is         :  78044913596398.78\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Result for  Proposed  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 99.94442493777689\n",
      "             Mean squared error is         :  4068201357772.2305\n",
      "=======================================================================\n",
      "Proposed  : For  680-bicg_output  and test size  20  === Maximum score between these models are: 0.9994442493777689\n",
      "Total time that elapsed in this expriment is:  438.24600000000004\n",
      "=======================================================================\n",
      "Training Result for  DT  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 98.58097839643446\n",
      "             Mean squared error is         :  103480583897051.19\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Result for  Proposed  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 99.91269764812087\n",
      "             Mean squared error is         :  6366427632488.179\n",
      "=======================================================================\n",
      "Proposed  : For  680-bicg_output  and test size  30  === Maximum score between these models are: 0.9991269764812087\n",
      "Total time that elapsed in this expriment is:  425.993\n",
      "=======================================================================\n",
      "Training Result for  DT  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 98.38624611521242\n",
      "             Mean squared error is         :  117333443505460.73\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Result for  Proposed  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 99.89152933118159\n",
      "             Mean squared error is         :  7886727469277.367\n",
      "=======================================================================\n",
      "Proposed  : For  680-bicg_output  and test size  40  === Maximum score between these models are: 0.9989152933118158\n",
      "Total time that elapsed in this expriment is:  432.923\n",
      "=======================================================================\n",
      "Training Result for  DT  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 97.2364901621746\n",
      "             Mean squared error is         :  200887931407857.62\n",
      "=======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n",
      "/home/amin/.local/lib/python3.6/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Result for  Proposed  and  680-bicg_output  is :\n",
      "             Train Score is                : % 100.0\n",
      "             Test Score is                 : % 99.86787622472283\n",
      "             Mean squared error is         :  9604478892002.115\n",
      "=======================================================================\n",
      "Proposed  : For  680-bicg_output  and test size  50  === Maximum score between these models are: 0.9986787622472283\n",
      "Total time that elapsed in this expriment is:  425.937\n",
      "=======================================================================\n",
      "[[[98.47, 99.96], [98.93, 99.94], [98.58, 99.91], [98.39, 99.89], [97.24, 99.87]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This code is for predicting the real value for Computation time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "# To use this experimental feature, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "N_SPLITS = 5\n",
    "TEST_SIZE = 10\n",
    "SEED = 7\n",
    "SCORE_TYPE = 'neg_mean_squared_error'\n",
    "\n",
    "def ReadBatch(directory):\n",
    "    df_list = []\n",
    "    for file in os.listdir(directory):\n",
    "        df = pd.read_csv(directory+file)\n",
    "        df_list.append(df)\n",
    "    final_df = pd.concat(df_list)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def ReadBatchHierarchy(walk_dir,exprimentName):\n",
    "    df_list = []\n",
    "    for root, subdirs, files in os.walk(walk_dir):\n",
    "        #print('--\\nroot = ' + root)\n",
    "        list_file_path = os.path.join(root, 'my-directory-list.txt')\n",
    "        #print('list_file_path = ' + list_file_path)\n",
    "\n",
    "        with open(list_file_path, 'wb') as list_file:\n",
    "            #for subdir in subdirs:\n",
    "                #print('\\t- subdirectory ' + subdir)\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                #print('\\t- file %s (full path: %s)' % (filename, file_path))\n",
    "                if (exprimentName in filename):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    if (\"Vega56\" in file_path): \n",
    "                        df['Arch']=7\n",
    "                        df['HW'] = 1\n",
    "                    if (\"TitanV\" in file_path): \n",
    "                        df['Arch']=5\n",
    "                        df['HW'] = 2\n",
    "                    if (\"P100\" in file_path): \n",
    "                        df['Arch']=2\n",
    "                        df['HW'] = 3\n",
    "                    if (\"K20\" in file_path): \n",
    "                        df['Arch']=4\n",
    "                        df['HW'] = 5\n",
    "                    if (\"2080Ti\" in file_path): \n",
    "                        df['Arch']=3\n",
    "                        df['HW'] = 6\n",
    "                    if (\"1070\" in file_path): \n",
    "                        df['Arch']=2\n",
    "                        df['HW'] = 7\n",
    "                    if (\"750\" in file_path): \n",
    "                        df['Arch']=1\n",
    "                        df['HW'] = 8\n",
    "                    if (\"680\" in file_path): \n",
    "                        df['Arch']=4\n",
    "                        df['HW'] = 9\n",
    "                    df_list.append(df)\n",
    "            os.remove(list_file_path)\n",
    "    \n",
    "    if df_list: \n",
    "        final_df = pd.concat(df_list)\n",
    "        return final_df\n",
    "\n",
    "    \n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    #xycoords = \"figure points\",\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "def model_plot(ScoreResults, ModelNames):\n",
    "    labels = ['10', '20', '30', '40', '50']\n",
    "    r1 = np.arange(len(labels))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    r2 = [(x - width/2) for x in r1]\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    for i in np.arange(NumberOfModels) :\n",
    "        data1 = [s[i] for s in ScoreResults]\n",
    "        model_name = [s[i] for s in ModelNames]\n",
    "        rect = ax.bar(r2 , data1, width/2, label=str(model_name[0]))\n",
    "        autolabel(rect)\n",
    "        r2 = [x - width/2 for x in r2]\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_xlabel('Percent')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(str(bench) + ' >> Scores by model and test data percent for ')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticks(y,minor=True)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "#benchmarks = ['nbody','bicg','coulomb_sum_2d','coulomb_sum_3d_iterative','fourier_32','fourier_50','fourier_64','fourier_91','fourier_128','fourier_197','gemm_output','hotspot','reduction']\n",
    "benchmarks = ['680-bicg_output']\n",
    "\n",
    "\n",
    "All_ScoreResults = []\n",
    "All_ModelNames   = []\n",
    "for bench in benchmarks:\n",
    "    ScoreResults = []\n",
    "    ModelNames = []\n",
    "    j=0\n",
    "    for testSize in range(TEST_SIZE, 60, 10):\n",
    "        #Reading all CSV files from different folders and add two Hardware model and Architecture columns to them\n",
    "        #Second parameters can be nbody, bicg, conv_3d, conv_output, coulomb_sum_2d, coulomb_sum_3d_iterative, coulomb_sum_3d,\n",
    "        #fourier_32, fourier_50, fourier_64, fourier_91, fourier_128, fourier_197, gemm_batch, gemm_output, hotspot, mtran, reduction, sort\n",
    "        \n",
    "        #data = ReadBatchHierarchy(\"/home/amin/mining/PC-GPU\",bench)\n",
    "        data = pd.read_csv(\"data/680-bicg_output.csv\")\n",
    "        # Here we split full dataset to X and Y which X is all features except Computation Time and Y is Computation Time\n",
    "        array = data.values\n",
    "\n",
    "        X = array[:,1:15] #First column is the name of benchmark and we remove it.\n",
    "        Y = array[:,15:]  # Using profiling counter variables as dependent variables\n",
    "        \n",
    "        #X = array[:,2:] #First column is the name of benchmark and we remove it.\n",
    "        #Y = array[:,1]  # Using profiling counter variables as dependent variables\n",
    "\n",
    "        \n",
    "        #Y = array[:,[1,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]]  # Using profiling counter variables as dependent variables\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=(testSize/100), random_state=SEED)\n",
    "        columns_names = list(data.columns)\n",
    "        columns_number = len(data.columns)\n",
    "\n",
    "        \n",
    "        # Estimate the score after iterative imputation of the missing values\n",
    "        # with different estimators\n",
    "        estimators = []\n",
    "        #estimators.append(('Bayes', BayesianRidge(compute_score=True)))\n",
    "        estimators.append(('DT', DecisionTreeRegressor(max_features='sqrt', random_state=0)))\n",
    "        estimators.append(('Proposed', ExtraTreesRegressor(n_estimators=10, random_state=0)))\n",
    "        #estimators.append(('Knn10', KNeighborsRegressor(n_neighbors=10)))\n",
    "        #estimators.append(('Knn30', KNeighborsRegressor(n_neighbors=30)))\n",
    "        #estimators.append(('Knn50', KNeighborsRegressor(n_neighbors=50)))\n",
    "        #estimators.append(('GB', GradientBoostingRegressor(n_estimators=100, max_depth=4,learning_rate=0.1, loss='huber',random_state=1)))\n",
    "        #estimators.append(('NN-logistic', MLPRegressor(activation='logistic',hidden_layer_sizes=(13,100,128,256,100,30,1))))\n",
    "        #estimators.append(('NN-relu', MLPRegressor(activation='relu')))\n",
    "        #estimators.append(('NN-tanh', MLPRegressor(activation='tanh')))\n",
    "        #estimators.append(('NN-identity', MLPRegressor(activation='identity')))\n",
    "        #estimators.append(('NN-relu-sgd', MLPRegressor(activation='relu', solver='sgd')))\n",
    "\n",
    "        NumberOfModels = len(estimators)\n",
    "        ExprimentStart = datetime.datetime.now()\n",
    "        start = 0\n",
    "        end = 0\n",
    "        maxScore = 0\n",
    "        Bestmodel = ''\n",
    "        ScoreResults.append([])\n",
    "        ModelNames.append([])\n",
    "        for name, imputer in estimators:\n",
    "            start = datetime.datetime.now()\n",
    "            imputer.fit(X_train, Y_train)\n",
    "            #saving model to a file for later usages\n",
    "            filename = \"SavedModel/\" + str(name) + \"_\" + str(bench) + \"_\" + str(testSize) + \".sav\"\n",
    "            pickle.dump(imputer, open(filename, 'wb'))\n",
    "            #Computing the score of model\n",
    "            scoreTrain = imputer.score(X_train, Y_train)\n",
    "            #compute time elapsed\n",
    "            end = datetime.datetime.now()\n",
    "            durationTrain = end - start\n",
    "            #Predicting with Test or X_test dataset and then writing the predicted results to .csv\n",
    "            start = datetime.datetime.now()\n",
    "            outputcsv = \"SavedPredictedCSV/\" + str(name) + \"_\" + str(bench) + \"_\" + str(testSize) + \".csv\"\n",
    "            outputcsvCompare = \"SavedPredictedCSV/\" + str(name) + \"_\" + str(bench) + \"_\" + str(testSize) + \"-forCompare.csv\"\n",
    "            predicted = imputer.predict(X_test)\n",
    "            pd.DataFrame(predicted).to_csv(outputcsv, sep=',')\n",
    "            pd.DataFrame(Y_test).to_csv(outputcsvCompare, sep=',') # Saving Y_test for comparison\n",
    "            scoreTest = imputer.score(X_test, Y_test)\n",
    "            if (scoreTest >= maxScore): \n",
    "                maxScore = scoreTest\n",
    "                Bestmodel = name\n",
    "            #for plot\n",
    "            ScoreResults[j].append(round(scoreTest*100,2))\n",
    "            ModelNames[j].append(name)\n",
    "            #####\n",
    "            end = datetime.datetime.now()\n",
    "            durationTest = end - start\n",
    "            #Print some reports\n",
    "            print(\"Training Result for \", name, \" and \", bench, \" is :\")\n",
    "            print(\"             Train Score is                : %\", scoreTrain * 100)\n",
    "            print(\"             Test Score is                 : %\", scoreTest * 100)\n",
    "            #print(\"             Time elapsed for Train is(ms) : \", durationTrain.total_seconds() * 1000)\n",
    "            #print(\"             Time elapsed for Test is(ms)  : \", durationTest.total_seconds() * 1000)\n",
    "            #print(\"             Max error is                  : \", max_error(Y_test, predicted))\n",
    "            print(\"             Mean squared error is         : \", mean_squared_error(Y_test, predicted))\n",
    "            #print(\"             Vriance score is(%)           : \", explained_variance_score(Y_test, predicted)) # # It was similar to Score\n",
    "            #print(\"             R^2 score is (best is 1.0)    : \", r2_score(Y_test, predicted))  # It was similar to Score\n",
    "            print(\"=======================================================================\")\n",
    "            \n",
    "            #features = list(range(2, columns_number-3))\n",
    "            \n",
    "            #features = list(range(2, 3))\n",
    "            #plot_partial_dependence(imputer, X_train, features,n_jobs=3, feature_names=columns_names, grid_resolution=50)\n",
    "            #fig = plt.gcf()\n",
    "            #fig.suptitle('Partial dependence of computation time on features-' + str(name))\n",
    "            #plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle\n",
    "\n",
    "        \n",
    "        ExprimentEnd = datetime.datetime.now()\n",
    "        TotalTime = ExprimentEnd - ExprimentStart\n",
    "        print(Bestmodel, ' : For ', str(bench), ' and test size ', testSize, ' === Maximum score between these models are:', maxScore)\n",
    "        print(\"Total time that elapsed in this expriment is: \", TotalTime.total_seconds() * 1000)\n",
    "        print(\"=======================================================================\")\n",
    "        j = j+1\n",
    "    \n",
    "    All_ScoreResults.append(ScoreResults)\n",
    "    All_ModelNames.append(ModelNames)\n",
    "    \n",
    "    #Plotting the result of each benchmark\n",
    "    labels = ['10', '20', '30', '40', '50']\n",
    "    r1 = np.arange(len(labels))  # the label locations\n",
    "    width = 0.20  # the width of the bars\n",
    "    r2 = [(x - width/2) for x in r1]\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    for i in np.arange(NumberOfModels) :\n",
    "        data1 = [s[i] for s in ScoreResults]\n",
    "        model_name = [s[i] for s in ModelNames]\n",
    "        rect = ax.bar(r2 , data1, width/2, label=str(model_name[0]))\n",
    "        autolabel(rect)\n",
    "        r2 = [x - width/2 for x in r2]\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_xlabel('Percent')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(str(bench) + ' >> Scores by model and test data percent for ')\n",
    "    #ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    #ax.set_yticks(y,minor=True)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "#Save the results of simulation\n",
    "FinalCSV = \"SavedPredictedCSV/\" + str(ExprimentEnd) + \"FinalResult-\"  + \".csv\"\n",
    "df = pd.DataFrame(columns=['Bench','Model','Test-10','Test-20','Test-30','Test-40','Test-50'])\n",
    "l=0\n",
    "print(All_ScoreResults)\n",
    "for bench, count in zip(benchmarks,np.arange(len(benchmarks))):\n",
    "    for i in np.arange(NumberOfModels) :\n",
    "            data1 = [s[i] for s in All_ScoreResults[count]]\n",
    "            model_Name = [s[i] for s in All_ModelNames[count]]\n",
    "            df.loc[l] = [str(bench),str(model_Name[0])] + list(data1)\n",
    "            l = l + 1\n",
    "df.to_csv(FinalCSV, sep=',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[265324, 2097152, 128, 2, 8, 1, 0, 0, 1, 0, 0, 1, 128, 32] => [4.0000000e+00 8.4563740e+06 5.8882400e+05 1.0000000e+00 1.2782162e+07\n",
      " 5.8982500e+05 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7396000e+01\n",
      " 2.3592960e+06 2.3765670e+06 0.0000000e+00 0.0000000e+00 2.4175600e-01\n",
      " 9.9859200e+01 1.3893632e+08 0.0000000e+00 2.3330816e+08 4.1943040e+06\n",
      " 3.5127296e+08 5.1380224e+07 0.0000000e+00 2.5378816e+07 1.9232300e+00\n",
      " 0.0000000e+00 1.0000000e+00 5.0000000e+00 0.0000000e+00 1.8130700e+01\n",
      " 1.0240000e+03 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2000000e+01] \n",
      "(expected [6 8566076 637542 2 12782362 720897 0 0 0.0 44.9362 2621440 2701805 0 0\n",
      " 0.671315 99.9302 142082048 0 265814016 8388608 361758720 101711872 0\n",
      " 29179904 3.09157 0 1 7 0 28.0958 1024 0 0 0 32]) \n",
      "[617585, 524288, 128, 2, 2, 0, 0, 1, 0, 0, 1, 1, 1024, 16] => [3.00000000e+00 8.81370700e+06 1.49325190e+07 2.00000000e+00\n",
      " 6.18767820e+07 2.98926090e+07 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.49181000e+01 2.14220800e+06 1.11001600e+06\n",
      " 0.00000000e+00 0.00000000e+00 9.91170000e-01 9.97078000e+01\n",
      " 1.64036608e+08 0.00000000e+00 7.62511360e+08 3.35544320e+07\n",
      " 3.70540544e+08 3.37510400e+08 0.00000000e+00 6.23370240e+07\n",
      " 4.54002000e-01 0.00000000e+00 1.00000000e+00 6.00000000e+00\n",
      " 0.00000000e+00 1.96045000e+01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00] \n",
      "(expected [3 8951568 14973891 2 61934102 30007297 0 0 0.0 35.4167 2097152 1048576 0\n",
      " 0 0.991137 99.5035 164036608 0 765853696 33554432 370540544 337510400 0\n",
      " 62529536 0.45127 0 1 6 0 18.9476 0 0 0 0 0]) \n",
      "[766183, 4194304, 128, 2, 2, 1, 0, 1, 0, 1, 0, 1, 128, 16] => [3.00000000e+00 8.60985300e+06 1.20043060e+07 2.00000000e+00\n",
      " 2.82489070e+07 3.08674570e+07 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.06250000e+01 6.29145600e+06 4.19430400e+06\n",
      " 0.00000000e+00 0.00000000e+00 9.91336000e-01 9.99768000e+01\n",
      " 1.67247872e+08 0.00000000e+00 9.80942848e+08 3.35544320e+07\n",
      " 4.47741952e+08 3.26107136e+08 0.00000000e+00 7.90036480e+07\n",
      " 6.73033000e-01 0.00000000e+00 1.00000000e+00 7.00000000e+00\n",
      " 0.00000000e+00 2.35046000e+01 1.02400000e+03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.20000000e+01] \n",
      "(expected [3 8752913 11305223 2 27789943 29949953 0 0 0.0 29.840999999999998 6651904\n",
      " 4685824 0 0 0.9948950000000001 99.9632 167247872 0 945291264 33554432\n",
      " 447741952 359661568 0 78217216 0.675335 0 1 7 0 23.5563 1024 0 0 0 32]) \n"
     ]
    }
   ],
   "source": [
    "np.printoptions(precision=5, suppress=True)\n",
    "filename = \"SavedModel/DT_680-bicg_output_50.sav\"\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "predictionsTest = loaded_model.predict(X_test)\n",
    "for i in range(3):\n",
    "    print('%s => %s \\n(expected %s) ' % (X_test[i].tolist(), predictionsTest[i], Y_test[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(node_numb, path, x):\n",
    "        path.append(node_numb)\n",
    "        if node_numb == x:\n",
    "            return True\n",
    "        left = False\n",
    "        right = False\n",
    "        if (children_left[node_numb] !=-1):\n",
    "            left = find_path(children_left[node_numb], path, x)\n",
    "        if (children_right[node_numb] !=-1):\n",
    "            right = find_path(children_right[node_numb], path, x)\n",
    "        if left or right :\n",
    "            return True\n",
    "        path.remove(node_numb)\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_rule(path, column_names):\n",
    "    mask = ''\n",
    "    for index, node in enumerate(path):\n",
    "        #We check if we are not in the leaf\n",
    "        if index!=len(path)-1:\n",
    "            # Do we go under or over the threshold ?\n",
    "            if (children_left[node] == path[index+1]):\n",
    "                mask += \"(df['{}']<= {}) \\t \".format(column_names[feature[node]], threshold[node])\n",
    "            else:\n",
    "                mask += \"(df['{}']> {}) \\t \".format(column_names[feature[node]], threshold[node])\n",
    "    # We insert the & at the right places\n",
    "    mask = mask.replace(\"\\t\", \"&\", mask.count(\"\\t\") - 1)\n",
    "    mask = mask.replace(\"\\t\", \"\")\n",
    "    return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz\n",
    "#!pip install ipywidgets\n",
    "\n",
    "import pickle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "iris = load_iris()\n",
    "\n",
    "feature_names = ['Computation duration (us)', 'Global size', 'Local size',\n",
    "       'FUSED', 'BICG_BATCH', 'USE_SHARED_MATRIX', 'USE_SHARED_VECTOR_1',\n",
    "       'USE_SHARED_VECTOR_2', 'USE_SHARED_REDUCTION_1',\n",
    "       'USE_SHARED_REDUCTION_2', 'ATOMICS', 'UNROLL_BICG_STEP',\n",
    "       'ROWS_PROCESSED', 'TILE']\n",
    "target_names = ['dram_utilization', 'dram_read_transactions',\n",
    "       'dram_write_transactions', 'l2_utilization', 'l2_read_transactions',\n",
    "       'l2_write_transactions', 'tex_utilization', 'tex_cache_transactions',\n",
    "       'local_memory_overhead', 'shared_efficiency',\n",
    "       'shared_load_transactions', 'shared_store_transactions',\n",
    "       'local_load_transactions', 'local_store_transactions',\n",
    "       'achieved_occupancy', 'sm_efficiency', 'inst_fp_32', 'inst_fp_64',\n",
    "       'inst_integer', 'inst_control', 'inst_compute_ld_st', 'inst_misc',\n",
    "       'inst_bit_convert', 'inst_executed', 'flop_sp_efficiency',\n",
    "       'flop_dp_efficiency', 'cf_fu_utilization', 'ldst_fu_utilization',\n",
    "       'tex_fu_utilization', 'issue_slot_utilization',\n",
    "       'Maximum work-group size', 'Local memory size', 'Private memory size',\n",
    "       'Constant memory size', 'Registers count']\n",
    "\n",
    "# Model (can also use single decision tree)\n",
    "array = data.values\n",
    "\n",
    "#X = array[:,2:] #First column is the name of benchmark and we remove it.\n",
    "#Y = array[:,1]\n",
    "\n",
    "\n",
    "X = array[:,1:15] #First column is the name of benchmark and we remove it.\n",
    "Y = array[:,15:]\n",
    "\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=(50/100), random_state=SEED)\n",
    "clf = clf.fit(X_test, Y_test)\n",
    "filename = \"SavedModel/680-bicg_output.sav\"\n",
    "pickle.dump(clf, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract rules from DT\n",
    "import pickle\n",
    "filename = \"SavedModel/680-bicg_output.sav\"\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "n_nodes = clf.tree_.node_count\n",
    "children_left = clf.tree_.children_left\n",
    "children_right = clf.tree_.children_right\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "\n",
    "# Leaves\n",
    "leave_id = clf.apply(X_test)\n",
    "\n",
    "paths ={}\n",
    "for leaf in np.unique(leave_id):\n",
    "    path_leaf = []\n",
    "    find_path(0, path_leaf, leaf)\n",
    "    paths[leaf] = np.unique(np.sort(path_leaf))\n",
    "\n",
    "rules = {}\n",
    "for key in paths:\n",
    "    rules[key] = get_rule(paths[key], ['Global size', 'Local size',\n",
    "       'FUSED', 'BICG_BATCH', 'USE_SHARED_MATRIX', 'USE_SHARED_VECTOR_1',\n",
    "       'USE_SHARED_VECTOR_2', 'USE_SHARED_REDUCTION_1',\n",
    "       'USE_SHARED_REDUCTION_2', 'ATOMICS', 'UNROLL_BICG_STEP',\n",
    "       'ROWS_PROCESSED', 'TILE', 'dram_utilization', 'dram_read_transactions',\n",
    "       'dram_write_transactions', 'l2_utilization', 'l2_read_transactions',\n",
    "       'l2_write_transactions', 'tex_utilization', 'tex_cache_transactions',\n",
    "       'local_memory_overhead', 'shared_efficiency',\n",
    "       'shared_load_transactions', 'shared_store_transactions',\n",
    "       'local_load_transactions', 'local_store_transactions',\n",
    "       'achieved_occupancy', 'sm_efficiency', 'inst_fp_32', 'inst_fp_64',\n",
    "       'inst_integer', 'inst_control', 'inst_compute_ld_st', 'inst_misc',\n",
    "       'inst_bit_convert', 'inst_executed', 'flop_sp_efficiency',\n",
    "       'flop_dp_efficiency', 'cf_fu_utilization', 'ldst_fu_utilization',\n",
    "       'tex_fu_utilization', 'issue_slot_utilization',\n",
    "       'Maximum work-group size', 'Local memory size', 'Private memory size',\n",
    "       'Constant memory size', 'Registers count'])\n",
    "\n",
    "print(rules)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "\n",
    "#graph = Source(tree.export_graphviz(clf, out_file=None, feature_names=feature_names, filled = True))\n",
    "#display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "\n",
    "#tree.export_graphviz(clf, out_file=\"rules.svg\", feature_names=feature_names, filled = True)\n",
    "\n",
    "# Export as dot file\n",
    "#dot_data = tree.export_graphviz(clf, out_file='tree.dot') \n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "#call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=3500'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "#from IPython.display import Image\n",
    "#Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHEN Global size <= 802480.5 && USE_SHARED_MATRIX > 3.0 && USE_SHARED_MATRIX > 6.0 && USE_SHARED_VECTOR_1 > 0.5 && USE_SHARED_MATRIX > 12.0 && Local size <= 786432.0 && USE_SHARED_REDUCTION_2 <= 0.5 && Global size <= 314177.5 && USE_SHARED_VECTOR_2 <= 0.5 && dram_utilization > 48.0 && Global size > 255941.0 && USE_SHARED_REDUCTION_1 > 0.5 && Global size <= 262909.0 && Global size <= 262164.0 && Global size > 260323.0 && ROWS_PROCESSED <= 0.5 THEN Y=6 (probability=nan) (values=[[3.00000000e+00]\n",
      " [8.41017600e+06]\n",
      " [2.33967100e+06]\n",
      " [1.00000000e+00]\n",
      " [1.37910190e+07]\n",
      " [6.47987300e+06]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [4.72316000e+01]\n",
      " [4.19430400e+06]\n",
      " [2.50061000e+06]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [2.49404000e-01]\n",
      " [9.97201000e+01]\n",
      " [1.37560064e+08]\n",
      " [0.00000000e+00]\n",
      " [2.35601920e+08]\n",
      " [4.19430400e+06]\n",
      " [3.48782592e+08]\n",
      " [3.55205120e+07]\n",
      " [0.00000000e+00]\n",
      " [2.55610880e+07]\n",
      " [1.15523000e+00]\n",
      " [0.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [4.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [1.33056000e+01]\n",
      " [1.02400000e+03]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [0.00000000e+00]\n",
      " [3.20000000e+01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import _tree\n",
    "import pickle\n",
    "filename = \"SavedModel/680-bicg_output.sav\"\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "node_indicator = clf.decision_path(X_train)\n",
    "n_nodes = clf.tree_.node_count\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "leave_id = clf.apply(X_train)\n",
    "feature_names  = ['Computation duration (us)','Global size', 'Local size',\n",
    "       'FUSED', 'BICG_BATCH', 'USE_SHARED_MATRIX', 'USE_SHARED_VECTOR_1',\n",
    "       'USE_SHARED_VECTOR_2', 'USE_SHARED_REDUCTION_1',\n",
    "       'USE_SHARED_REDUCTION_2', 'ATOMICS', 'UNROLL_BICG_STEP',\n",
    "       'ROWS_PROCESSED', 'TILE', 'dram_utilization', 'dram_read_transactions',\n",
    "       'dram_write_transactions', 'l2_utilization', 'l2_read_transactions',\n",
    "       'l2_write_transactions', 'tex_utilization', 'tex_cache_transactions',\n",
    "       'local_memory_overhead', 'shared_efficiency',\n",
    "       'shared_load_transactions', 'shared_store_transactions',\n",
    "       'local_load_transactions', 'local_store_transactions',\n",
    "       'achieved_occupancy', 'sm_efficiency', 'inst_fp_32', 'inst_fp_64',\n",
    "       'inst_integer', 'inst_control', 'inst_compute_ld_st', 'inst_misc',\n",
    "       'inst_bit_convert', 'inst_executed', 'flop_sp_efficiency',\n",
    "       'flop_dp_efficiency', 'cf_fu_utilization', 'ldst_fu_utilization',\n",
    "       'tex_fu_utilization', 'issue_slot_utilization',\n",
    "       'Maximum work-group size', 'Local memory size', 'Private memory size',\n",
    "       'Constant memory size', 'Registers count']\n",
    "\n",
    "def value2prob(value):\n",
    "    return value / value.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def print_condition(sample_id):\n",
    "    print(\"WHEN\", end=' ')\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                        node_indicator.indptr[sample_id + 1]]\n",
    "    for n, node_id in enumerate(node_index):\n",
    "        if leave_id[sample_id] == node_id:\n",
    "            values = clf.tree_.value[node_id]\n",
    "            probs = value2prob(values)\n",
    "            print('THEN Y={} (probability={}) (values={})'.format(\n",
    "                probs.argmax(), probs.max(), values))\n",
    "            continue\n",
    "        if n > 0:\n",
    "            print('&& ', end='')\n",
    "        if (X_train[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "            threshold_sign = \"<=\"\n",
    "        else:\n",
    "            threshold_sign = \">\"\n",
    "        if feature[node_id] != _tree.TREE_UNDEFINED:\n",
    "            print(\n",
    "                \"%s %s %s\" % (\n",
    "                    feature_names[feature[node_id]],\n",
    "                    #Xtrain[sample_id,feature[node_id]] # actual value\n",
    "                    threshold_sign,\n",
    "                    threshold[node_id]),\n",
    "                end=' ')\n",
    "            \n",
    "print_condition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9425023571352187\n"
     ]
    }
   ],
   "source": [
    "#Using Gradient Boosting Regressor\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "from sklearn import ensemble\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "array = data.values\n",
    "\n",
    "X = array[:,1:15] #First column is the name of benchmark and we remove it.\n",
    "Y = array[:,15:]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=(50/100), random_state=SEED)\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "\n",
    "clfMO = MultiOutputRegressor(ensemble.GradientBoostingRegressor(**params),n_jobs=-1)\n",
    "clfMO.fit(X_train, Y_train)\n",
    "\n",
    "score = clfMO.score(X_test, Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
